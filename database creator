# mostly written by ai, i can't take credit
import requests
from bs4 import BeautifulSoup
import sys
import urllib.parse
import re
import os


def extract_links_and_text(url):
    """
    Extract all hyperlinks and their anchor text from a given URL.
    
    Args:
        url (str): The URL to extract links from
        
    Returns:
        list of tuples: Each tuple contains (anchor_text, absolute_url)
    """
    try:
        print(f"Sending HTTP GET request to {url}")
        response = requests.get(url)
        if response.status_code != 200:
            print(f"Failed to retrieve content. Status code: {response.status_code}")
            return []
        soup = BeautifulSoup(response.content, 'html.parser')
        links_and_text = []
        for link in soup.find_all('a'):
            href = link.get('href')
            if href:
                absolute_url = urllib.parse.urljoin(url, href)
                anchor_text = link.get_text(strip=True)
                links_and_text.append((anchor_text, absolute_url))
        print(f"Found {len(links_and_text)} links on the page")
        return links_and_text
    except requests.exceptions.RequestException as e:
        print(f"Error during request: {e}")
        return []
    
def get_file_info(url):
    # Send a HEAD request to fetch headers only
    response = requests.head(url, allow_redirects=True)
    headers = response.headers

    # Try to get filename from Content-Disposition header
    cd = headers.get('Content-Disposition')
    filename = None
    if cd:
        fname_match = re.findall('filename="?([^"]+)"?', cd)
        if fname_match:
            filename = fname_match[0]

    # Fallback: extract filename from URL
    if not filename:
        path = urllib.parse.urlparse(response.url).path
        filename = path.split('/')[-1] or 'downloaded_file'
        
    return filename

def create_db(links_and_text, filename, filetype):
    if os.path.exists(filename):
        os.remove(filename)
        print(f"deleting and recreating {filename}")
    with open(filename, "a") as f:
        num = len(links_and_text)
        count = 0
        for i in links_and_text:
            filename = get_file_info(i[1])
            if not filename:
                print(f"could not get filename for url: {i[1]}")
                continue
            line = f"{i[0]}	{filetype}	ANY	idk	idk	{i[1]}	{filename}	69	https://www.pngmart.com/image/tag/funny"
            f.write(line + "\n")
            count += 1
            print(f"{count} / {num}: {round(count / num * 100, 1)}% done!")
        
            
        
def main():
    if len(sys.argv) > 1:
        url = sys.argv[1]
    else:
    	# change information here as needed.
        url = "https://myrient.erista.me/files/No-Intro/Nintendo%20-%20Nintendo%20DS%20%28Decrypted%29/?C=N&O=D"
        filename = "output.txt"
        filetype = "nds"
        links_and_text = extract_links_and_text(url)
    if links_and_text:
        create_db(links_and_text, filename, filetype)
    else:
        print("No links were extracted.")

if __name__ == "__main__":
    main()
